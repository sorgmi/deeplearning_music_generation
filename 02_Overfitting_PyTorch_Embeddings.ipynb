{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting Example\n",
    "We try to overfit a single simple song with a encoder-decoder LSTM.\n",
    "This time we use PyTorch and embeddings.\n",
    "The code is inspired by this tutorial: [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note, metadata\n",
    "\n",
    "def get():\n",
    "    piece = stream.Score()\n",
    "    p1 = stream.Part()\n",
    "    p1.id = 'part1'\n",
    "\n",
    "    notes = [note.Note('C4', type='quarter'),\n",
    "             note.Note('D4', type='quarter'),\n",
    "             note.Note('E4', type='quarter'),\n",
    "             note.Note('F4', type='quarter'),\n",
    "             note.Note('G4', type='half'),\n",
    "             note.Note('G4', type='half'),\n",
    "    \n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('G4', type='half'),\n",
    "\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('A4', type='quarter'),\n",
    "             note.Note('G4', type='half'),\n",
    "\n",
    "             note.Note('F4', type='quarter'),\n",
    "             note.Note('F4', type='quarter'),\n",
    "             note.Note('F4', type='quarter'),\n",
    "             note.Note('F4', type='quarter'),\n",
    "             note.Note('E4', type='half'),\n",
    "             note.Note('E4', type='half'),\n",
    "\n",
    "             note.Note('D4', type='quarter'),\n",
    "             note.Note('D4', type='quarter'),\n",
    "             note.Note('D4', type='quarter'),\n",
    "             note.Note('D4', type='quarter'),\n",
    "             note.Note('C4', type='half')\n",
    "            ]\n",
    "    p1.append(notes)\n",
    "    piece.insert(0, metadata.Metadata())\n",
    "    piece.metadata.title = 'Alle meine Entchen'\n",
    "    piece.insert(0, p1)\n",
    "    return piece, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv223'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv223');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQABBABNVHJrAAABAAD/AwAA4ABAAJA8WogAgDwAAJA+WogAgD4AAJBAWogAgEAAAJBBWogAgEEAAJBDWpAAgEMAAJBDWpAAgEMAAJBFWogAgEUAAJBFWogAgEUAAJBFWogAgEUAAJBFWogAgEUAAJBDWpAAgEMAAJBFWogAgEUAAJBFWogAgEUAAJBFWogAgEUAAJBFWogAgEUAAJBDWpAAgEMAAJBBWogAgEEAAJBBWogAgEEAAJBBWogAgEEAAJBBWogAgEEAAJBAWpAAgEAAAJBAWpAAgEAAAJA+WogAgD4AAJA+WogAgD4AAJA+WogAgD4AAJA+WogAgD4AAJA8WpAAgDwAiAD/LwA=');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "piece, notes = get()\n",
    "piece.show('midi')\n",
    "#piece.show() # doesn't work inside the notebook for me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/overfitting_piece.PNG](images/overfitting_piece.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Encoding & Data Preparation\n",
    "\n",
    "We use:\n",
    "- 128 midi notes\n",
    "- 128 additional midi notes. This represents the midi notes which are Tied to the previous note.\n",
    "- 258 additional symbols (Start, Stop)\n",
    "\n",
    "Therefore we encode our notes as 131-dimensional vector.\n",
    "\n",
    "* The encoder get's hald of the song as input\n",
    "* the decoder has to produce the missing half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "from music21 import pitch, interval, stream\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalTokens():\n",
    "    return 128*2+ 2  # 128 midi notes + Start + Stop\n",
    "\n",
    "def getStartIndex():\n",
    "    return 256\n",
    "\n",
    "def getStopIndex():\n",
    "    return 257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeNoteList(notes, delta):\n",
    "    sequence = []\n",
    "\n",
    "    for n in notes:\n",
    "        if (n.isNote):\n",
    "            sequence.append(n.pitch.midi)\n",
    "            ticksOn = int(n.duration.quarterLength / delta)\n",
    "            #print(\"ticksOn:\", ticksOn)\n",
    "            for i in range(0, ticksOn-1):\n",
    "                sequence.append(n.pitch.midi + 128)\n",
    "\n",
    "        if (n.isChord):\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def split(notes, splitRatio=0.5):\n",
    "    splitIndex = int(len(notes)*splitRatio)\n",
    "    x = notes[0:splitIndex]\n",
    "    y = notes[splitIndex:] + [getStopIndex()]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([60, 62, 64, 65, 67, 195, 67, 195, 69, 69, 69, 69, 67, 195, 69, 69], [69, 69, 67, 195, 65, 65, 65, 65, 64, 192, 64, 192, 62, 62, 62, 62, 60, 188, 257])]\n"
     ]
    }
   ],
   "source": [
    "input = encodeNoteList(notes, delta=1)\n",
    "input, target = split(input, splitRatio=0.49)\n",
    "pairs = [(input,target)]\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model definition & Training\n",
    "We use a encoder-decoder model from here: [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher_forcing_ratio = 1.0\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, quatsch):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, quatsch\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "encoder = EncoderRNN(getTotalTokens(), hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, getTotalTokens()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = len(input)\n",
    "    target_length = len(target)\n",
    "\n",
    "    input = torch.tensor(input)\n",
    "    target = torch.tensor(target).view(-1, 1)\n",
    "\n",
    "    encoder_outputs = torch.zeros(target_length+max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[getStartIndex()]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target[di])\n",
    "            decoder_input = target[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target[di])\n",
    "            if decoder_input.item() == getStopIndex():\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(pairs, encoder, decoder, epochs, learning_rate=0.01):\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(0, epochs):\n",
    "        for example in range(0, len(pairs)):\n",
    "            training_pair = pairs[example]\n",
    "            input = training_pair[0]\n",
    "            target = training_pair[1]\n",
    "\n",
    "            loss = train(input, target,encoder,decoder, encoder_optimizer, decoder_optimizer,criterion)\n",
    "\n",
    "        print(\"Epoch\", iter+1, \" finished. Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  finished. Loss:  5.579245316354852\n",
      "Epoch 2  finished. Loss:  5.283050537109375\n",
      "Epoch 3  finished. Loss:  4.988585622687089\n",
      "Epoch 4  finished. Loss:  4.669613486842105\n",
      "Epoch 5  finished. Loss:  4.309282804790296\n",
      "Epoch 6  finished. Loss:  3.9054758172286186\n",
      "Epoch 7  finished. Loss:  3.4812493575246712\n",
      "Epoch 8  finished. Loss:  3.0921381900185034\n",
      "Epoch 9  finished. Loss:  2.7919518320184005\n",
      "Epoch 10  finished. Loss:  2.5700984754060445\n",
      "Epoch 11  finished. Loss:  2.3858753003572164\n",
      "Epoch 12  finished. Loss:  2.222306100945724\n",
      "Epoch 13  finished. Loss:  2.0761546084755347\n",
      "Epoch 14  finished. Loss:  1.9452940288342928\n",
      "Epoch 15  finished. Loss:  1.8270845915141858\n",
      "Epoch 16  finished. Loss:  1.719544862446032\n",
      "Epoch 17  finished. Loss:  1.6216381474545127\n",
      "Epoch 18  finished. Loss:  1.5327499791195518\n",
      "Epoch 19  finished. Loss:  1.4522033490632709\n",
      "Epoch 20  finished. Loss:  1.3791188691791736\n",
      "Epoch 21  finished. Loss:  1.3125279075221012\n",
      "Epoch 22  finished. Loss:  1.2515234696237665\n",
      "Epoch 23  finished. Loss:  1.1953461295679997\n",
      "Epoch 24  finished. Loss:  1.1433908562911184\n",
      "Epoch 25  finished. Loss:  1.0951760944567228\n",
      "Epoch 26  finished. Loss:  1.0503151542262028\n",
      "Epoch 27  finished. Loss:  1.0084881029630963\n",
      "Epoch 28  finished. Loss:  0.9694189774362665\n",
      "Epoch 29  finished. Loss:  0.9328650424354955\n",
      "Epoch 30  finished. Loss:  0.8986101652446546\n",
      "Epoch 31  finished. Loss:  0.8664601978502775\n",
      "Epoch 32  finished. Loss:  0.8362400657252261\n",
      "Epoch 33  finished. Loss:  0.8077922620271382\n",
      "Epoch 34  finished. Loss:  0.7809756429571855\n",
      "Epoch 35  finished. Loss:  0.7556626169305098\n",
      "Epoch 36  finished. Loss:  0.731738241095292\n",
      "Epoch 37  finished. Loss:  0.7090994182385897\n",
      "Epoch 38  finished. Loss:  0.6876526882773951\n",
      "Epoch 39  finished. Loss:  0.6673135255512438\n",
      "Epoch 40  finished. Loss:  0.6480053349545127\n",
      "Epoch 41  finished. Loss:  0.6296582472951788\n",
      "Epoch 42  finished. Loss:  0.6122089185212788\n",
      "Epoch 43  finished. Loss:  0.5955996262399774\n",
      "Epoch 44  finished. Loss:  0.5797767639160156\n",
      "Epoch 45  finished. Loss:  0.564691844739412\n",
      "Epoch 46  finished. Loss:  0.5502998954371402\n",
      "Epoch 47  finished. Loss:  0.5365597072400545\n",
      "Epoch 48  finished. Loss:  0.5234323802747225\n",
      "Epoch 49  finished. Loss:  0.5108824278178968\n",
      "Epoch 50  finished. Loss:  0.4988768728155839\n",
      "Epoch 51  finished. Loss:  0.48738449498226766\n",
      "Epoch 52  finished. Loss:  0.476376684088456\n",
      "Epoch 53  finished. Loss:  0.46582573338558797\n",
      "Epoch 54  finished. Loss:  0.45570704811497736\n",
      "Epoch 55  finished. Loss:  0.4459962844848633\n",
      "Epoch 56  finished. Loss:  0.43667110643888774\n",
      "Epoch 57  finished. Loss:  0.4277107841090152\n",
      "Epoch 58  finished. Loss:  0.41909529033460113\n",
      "Epoch 59  finished. Loss:  0.41080627943340103\n",
      "Epoch 60  finished. Loss:  0.4028266856544896\n",
      "Epoch 61  finished. Loss:  0.3951396942138672\n",
      "Epoch 62  finished. Loss:  0.38773009651585627\n",
      "Epoch 63  finished. Loss:  0.38058326118870783\n",
      "Epoch 64  finished. Loss:  0.37368586188868474\n",
      "Epoch 65  finished. Loss:  0.36702507420590047\n",
      "Epoch 66  finished. Loss:  0.3605891779849404\n",
      "Epoch 67  finished. Loss:  0.35436615191007914\n",
      "Epoch 68  finished. Loss:  0.3483462584646125\n",
      "Epoch 69  finished. Loss:  0.34251915781121506\n",
      "Epoch 70  finished. Loss:  0.3368756896571109\n",
      "Epoch 71  finished. Loss:  0.3314069948698345\n",
      "Epoch 72  finished. Loss:  0.3261048166375411\n",
      "Epoch 73  finished. Loss:  0.3209612495020816\n",
      "Epoch 74  finished. Loss:  0.31596906561600535\n",
      "Epoch 75  finished. Loss:  0.31112128809878703\n",
      "Epoch 76  finished. Loss:  0.30641129142359685\n",
      "Epoch 77  finished. Loss:  0.3018330523842259\n",
      "Epoch 78  finished. Loss:  0.29738049758108037\n",
      "Epoch 79  finished. Loss:  0.2930486076756528\n",
      "Epoch 80  finished. Loss:  0.2888316104286595\n",
      "Epoch 81  finished. Loss:  0.28472503862882914\n",
      "Epoch 82  finished. Loss:  0.28072397332442434\n",
      "Epoch 83  finished. Loss:  0.2768241982710989\n",
      "Epoch 84  finished. Loss:  0.27302134664435135\n",
      "Epoch 85  finished. Loss:  0.26931135277999074\n",
      "Epoch 86  finished. Loss:  0.2656908537212171\n",
      "Epoch 87  finished. Loss:  0.2621556834170693\n",
      "Epoch 88  finished. Loss:  0.25870313142475326\n",
      "Epoch 89  finished. Loss:  0.2553296340139289\n",
      "Epoch 90  finished. Loss:  0.2520320791947214\n",
      "Epoch 91  finished. Loss:  0.2488076561375668\n",
      "Epoch 92  finished. Loss:  0.24565385517321134\n",
      "Epoch 93  finished. Loss:  0.24256751411839536\n",
      "Epoch 94  finished. Loss:  0.2395465248509457\n",
      "Epoch 95  finished. Loss:  0.23658877924868935\n",
      "Epoch 96  finished. Loss:  0.2336916672556024\n",
      "Epoch 97  finished. Loss:  0.2308531309428968\n",
      "Epoch 98  finished. Loss:  0.22807146373548007\n",
      "Epoch 99  finished. Loss:  0.22534440693102384\n",
      "Epoch 100  finished. Loss:  0.22267040453459086\n"
     ]
    }
   ],
   "source": [
    "trainIters(pairs, encoder, decoder, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Inference\n",
    "* Use the trained model and predict the second ahlf of the training data\n",
    "* Represent the generated as a music21 piece in order to display and play it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor(input)\n",
    "    input_length = input_tensor.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    max_length = 25\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[getStartIndex()]], device=device)  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        #decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        if topi.item() == getStopIndex():\n",
    "            decoded_words.append(getStopIndex())\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(topi.item())\n",
    "\n",
    "        decoder_input = topi.squeeze().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeSequence(seq, input=None, delta=1):\n",
    "    notes = []\n",
    "\n",
    "    for i in range(0, len(seq)):\n",
    "\n",
    "        index = seq[i]\n",
    "\n",
    "        if index == getStopIndex():\n",
    "            break\n",
    "\n",
    "        if i == 0 and index <= 128:\n",
    "            n = music21.note.Note()\n",
    "            n.pitch.midi = index\n",
    "            notes.append(n)\n",
    "        elif i == 0:\n",
    "            print(index)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        else:\n",
    "            previousNote = notes[-1].pitch.midi\n",
    "\n",
    "            if index <= 128:\n",
    "                n = music21.note.Note()\n",
    "                n.pitch.midi = index\n",
    "                notes.append(n)\n",
    "            elif index < 128 * 2 and index - 128 == previousNote:\n",
    "                notes[-1].quarterLength += delta\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "    if input is not None:\n",
    "        print(\"reiin\", input)\n",
    "        notes = input + [music21.note.Rest(type='half')] + notes\n",
    "\n",
    "    piece = music21.stream.Score()\n",
    "    p1 = music21.stream.Part()\n",
    "    p1.id = 'part1'\n",
    "\n",
    "    p1.append(notes)\n",
    "    piece.insert(0, music21.metadata.Metadata())\n",
    "    piece.metadata.title = 'Title'\n",
    "    piece.insert(0, p1)\n",
    "    return piece, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: [60, 62, 64, 65, 67, 195, 67, 195, 69, 69, 69, 69, 67, 195, 69, 69]\n",
      "d: [69, 69, 67, 195, 65, 65, 65, 65, 64, 192, 64, 192, 62, 62, 62, 62, 62, 60, 188, 257]\n",
      "t: [69, 69, 67, 195, 65, 65, 65, 65, 64, 192, 64, 192, 62, 62, 62, 62, 60, 188, 257]\n"
     ]
    }
   ],
   "source": [
    "inputNotes = notes[:int(len(notes)*0.49)]\n",
    "\n",
    "print(\"i:\", input)\n",
    "print(\"d:\", decoded_words)\n",
    "print(\"t:\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reiin [<music21.note.Note C>, <music21.note.Note D>, <music21.note.Note E>, <music21.note.Note F>, <music21.note.Note G>, <music21.note.Note G>, <music21.note.Note A>, <music21.note.Note A>, <music21.note.Note A>, <music21.note.Note A>, <music21.note.Note G>, <music21.note.Note A>, <music21.note.Note A>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv476'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv476');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQABBABNVHJrAAABCgD/AwAA4ABAAJA8WogAgDwAAJA+WogAgD4AAJBAWogAgEAAAJBBWogAgEEAAJBDWpAAgEMAAJBDWpAAgEMAAJBFWogAgEUAAJBFWogAgEUAAJBFWogAgEUAAJBFWogAgEUAAJBDWpAAgEMAAJBFWogAgEUAAJBFWogAgEUAkACQRVqIAIBFAACQRVqIAIBFAACQQ1qQAIBDAACQQVqIAIBBAACQQVqIAIBBAACQQVqIAIBBAACQQVqIAIBBAACQQFqQAIBAAACQQFqQAIBAAACQPlqIAIA+AACQPlqIAIA+AACQPlqIAIA+AACQPlqIAIA+AACQPlqIAIA+AACQPFqQAIA8AIgA/y8A');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p, _ = decodeSequence(decoded_words, inputNotes, delta=1)\n",
    "p.show('midi')\n",
    "#p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/overfitting_pytorch.PNG](images/overfitting_pytorch.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder perfectly reproduced the piece!\n",
    "\n",
    "## Limitations & future work\n",
    "- [ ] No chords supported only single notes\n",
    "- [ ] No Ties between different notes supported\n",
    "- [ ] Use Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
